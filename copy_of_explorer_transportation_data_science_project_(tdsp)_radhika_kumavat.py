# -*- coding: utf-8 -*-
"""Copy of EXPLORER - Transportation Data Science Project (TDSP) - Radhika Kumavat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DtuYq5YT3OGGYytQ7v4vkXJTGigqyluN

# ðŸš— **Welcome to the <font color='crimson'>** **Explorer Transportation Data Science Project! ðŸš—**</font>
 Hosted by the [Northeast Big Data Innovation Hub](https://nebigdatahub.org/about) & [National Student Data Corps](https://nebigdatahub.org/nsdc), in collaboration with the [U.S. Department of Transportation Federal Highway Administration](https://highways.dot.gov/).


---

## <font color='crimson'>**Project Information and Background:**</font>

**Project Description:**

By participating in this project, you are joining a
community of transportation data science learners interested in making roads safer for vulnerable road users.

The Explorer TDSP has six Milestones, including guided transportation research into a community of interest. Each Milestone can take 1-5 hours, or less, depending on your level of experience.

To learn more about this project, including key highlights, incentives, and important links, [review the TDSP Webpage here](https://nebigdatahub.org/nsdc/tdsp/)!

## <font color='crimson'>**How to Get Started:**</font>

In order to work within this Google Colab Notebook, **please start by clicking on "File" in the top left corner of your notebook, and then "Save a copy in Drive." Rename the file to "Explorer TDSP - Your Full Name."** This will save a copy of the notebook in your personal Google Drive.

You may now begin!

---
---

## <font color='crimson'>**A Quick Introduction to Google Colab**</font>

Read below for a Google Colab QuickStart:
- Google Colab is a Python Notebook environment built by Google that's free for all.
- Colab Notebooks are made up of cells; cells can be either *text* or *code* cells. You can click the +code or +text button at the top of the Notebook to create a new cell.
- Text cells use a format called [Markdown](https://www.markdownguide.org/getting-started/). Knowledge of Markdown is not required for this project. However, if you'd like to learn more, [check out this Cheatsheet!](https://www.markdownguide.org/cheat-sheet/)
- Python code is executed in *code* cells. When you want to run your code, hover your cursor over the square brackets in the top left corner of your code cell. You'll notice a play button pop up! (â–¶) Click the play button to run the code in that cell. Code cells run one at a time.
- The memory shared across your notebook is called the *Runtime*. You can think of a Runtime as a "code session" where everything you create and execute is temporarily stored.
- Runtimes will persist for a short period of time, so you are safe if you need to refresh the page, but Google will shutdown a Runtime after enough time has passed. Everything that you have printed out will remain within your Notebook even if the runtime is disconnected.

If this is your first time using Google Colab, we highly recommend reviewing the [NSDC's *Using Google Colab Guide*](https://nebigdatahub.org/wp-content/uploads/2023/04/NSDC-Data-Science-Projects-Introduction-Using-Google-Colab.pdf) before continuing. For a more comprehensive guide, see [Colab's *Welcome To Colaboratory* walkthrough.](https://colab.research.google.com/github/prites18/NoteNote/blob/master/Welcome_To_Colaboratory.ipynb)

## <font color='crimson'>**An Introduction to Python Programming**</font>

Python is a programming language often used to analyze data.

Python is open-source, which means it's free to use and distribute, even for commercial purposes. Python's versatility allows it to be used for web development, data visualization, artificial intelligence, scientific computing, and more.

Python's extensive standard library, along with its powerful third-party packages, enable developers and data scientists to perform a vast array of tasks.

For those looking to dive deeper into Python, here are some valuable resources:
- [The Official Python Documentation](https://docs.python.org/3/) â€“ Offers comprehensive guides and reference materials for Python leaners.
- [Real Python](https://realpython.com/) â€“ Provides tutorials and articles for Python developers of all skill levels.
- [PyCon](https://pycon.org/) â€“ The largest annual gathering for the Python community, which is useful for learning from experts and discovering the latest developments in the Python ecosystem.
- [Python for Everybody](https://www.py4e.com/) â€“ A book and website by Dr. Charles Severance that offers a free course on Python for beginners.

**Let's review some essential Python Functions!**

Here are some key functions you'll frequently encounter:

1. **`head()`**: This function is crucial for getting a quick overview of your dataset. By default, it returns the first five rows, offering a snapshot of your data's structure and values.

2. **`describe()`**: This provides a summary of the statistical characteristics of your dataset. It's particularly useful for gaining insights into the distribution, mean, standard deviation, and range of numerical columns.

3. **`sum()`**: This calculates the total sum of a column or a series of numbers, proving essential for quick calculations and aggregations in data analysis.

4. **`isnull()`**: This helps identify missing or null values in your dataset, allowing for effective data cleaning and preprocessing.

5. **`value_counts()`**: Understanding the frequency of various values in your dataset is a common task in data science. The `value_counts()` function makes this easy by counting the occurrence of each unique value in a column.

Now that you've reviewed these important concepts, let's dive in to the project!

## <font color='crimson'>**Milestone #1 - Data Preparation**</font>
GOAL: The main goal of this milestone is to set up your environment, install the required packages, learn how to access data and do some basic exploratory data analysis.

**Step 1:** Setting up libraries and installing packages

A **library** is a collection of code that you can use in your programs, while a **package** is a folder that contains libraries or other packages, organized for easy use.

To install a library, we'll use the following format:
```python
 import <library name> as <shortname>
```
We use a *short name* since it is easier to refer to the package to access functions and also to refer to subpackages within the library. Think of it as a nickname for easier reference!
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import folium

"""These are the libraries that will help us throughout this project. We invite you to research each library for a better understanding.

We encourage you to read more about the important and most commonly used libraries like Pandas, Matplotlib, and Seaborn and write a few lines in your own words about what they do. [You may use the Data Science Resource Repository (DSRR) to find resources to get started!](https://nebigdatahub.org/nsdc/data-science-resource-repository/)

**TO DO:** Write a few lines about what each library does:




> * Pandas: Pandas is a powerful library for data manipulation and analysis. It provides two primary data structures:Series, DataFrames
> * Matplotlib: Matplotlib is the base library for creating static, animated, and interactive visualizations in Python. It provides fine-grained control over plots and works well with Pandas DataFrames.
> * Seaborn:Seaborn is built on top of Matplotlib and makes it easier to create statistically meaningful plots. It integrates well with Pandas and provides beautiful themes by default. It is often used for visualizing trends and relationships in complex datasets.

**Step 2:** Letâ€™s access our data. We will be using the [NYC OpenData Motor Vehicle Collisions - Crashes dataset](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95). According to NYC Open Data, "each row represents a crash event. The Motor Vehicle Collisions data tables contain information from all police reported motor vehicle collisions in NYC." If you need a reminder on how to upload your dataset, [please review helpful hints here.](https://nebigdatahub.org/wp-content/uploads/2023/04/NSDC-Data-Science-Projects-Introduction-Using-Google-Colab.pdf)

Since this is a large dataset, we highly recommend that you upload your data by [mounting your Google Drive](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA).

To mount your Google Drive, begin by finding the folder icon on the left side of your screen. When you click on this folder icon, you will open the Files window. Click on the icon at the top of the Files window that says "Mount Drive" as you hover over it.
"""

from google.colab import drive
drive.mount('/content/drive')

"""Next, we will read the data using the `pd.read_csv` function.
Ensure that you've downloaded the dataset from NYC OpenData, and uploaded the dataset to your Google Drive.

Hint: Your data's file path describes the location of where your data lives. To locate your data's file path, click on the folder/file-shaped icon on the left side of the Notebook. You'll notice a folder labeled "drive." Search for your Motor Vehicle Collisions Dataset within that folder. Right click on your Dataset to "copy path." Paste that path below.
"""

# TODO: Read the data using pandas read_csv function
data = pd.read_csv("/content/drive/My Drive/Masters Project/datasets.csv")

"""**Step 3:** Let's see what the data looks like. We can use the `head` function which returns the first 5 rows of the dataset."""

# TODO: Print the first 5 rows of the data using head function of pandas
data.head()

# TODO: Describe the data using the describe function of pandas
desc_stats = data.describe()
desc_stats

"""The information above is currently formatted in scientific notation. Need a refresher? [Review how to analyze and convert to and from scientific notation here!](https://www.mathsisfun.com/numbers/scientific-notation.html)

1. Latitude & Longitude: The latitude and longitude indicate where the crashes are occurring. However, there are some data points with latitude and longitude values of 0, which is likely due to missing or inaccurate data.

2. Number of Persons Injured: On average, each crash has around 0.305 injuries. The maximum number of injuries in a single crash is 43.

3. Number of Persons Killed: Fatalities are rare, with an average of 0.00146 deaths per crash. The maximum number of deaths in one crash is 8.

4. Number of Pedestrians, Cyclists, and Motorists Injured/Killed: These columns provide a breakdown of the injuries and fatalities by type of individual involved.

5. Collision ID: This is a unique identifier for each crash.

---

##<font color='crimson'> **Milestone #2 - Data Ethics, Pre-Processing, and Exploration** </font>
GOAL: The main goal of this milestone is to assess the dataset, find missing values, and decide what to do with those missing data points.

**Step 1:**
Before we begin assessing our data for missing values, it is important that we understand the ethical implications surrounding data processing. To best prepare yourself for this module, review one or more of the following resources:
- [Data Science Ethics Flashcard Video Series](https://youtube.com/playlist?list=PLNs9ZO9jGtUB7XTjXy-ttoo2QSLld9SrV&feature=shared)
- [What Do I Need to Understand about Data Ethics?](https://www.youtube.com/watch?v=Efy8htCDueE)
-[Introduction to Data Cleaning](https://www.youtube.com/watch?v=t8WkoGLkdTk)

**TO DO:** Based on the resources above and outside knowledge, what are some potential bias issues related to the availability of data from well-resourced communities as compared to under-resourced communities? How might bias show up in our dataset?

> Answer here: To start with I can state some potential bias issue by comparing well- resources and under-resources communities.
1. Data accessibility: Well- resources communities can have better access to technology, internet, etc. which can lead to richer more comprehensive datasets whereas under-resources communities can face barriers like lack of technology, lower internet access, etc. can effort their data sparse or missing altogether.
2. Quality of Data: well- resources communities data collection may be of higher quality due to more sophisticated tools and methodologies, which can later result in data accuracy, whereas under- resources communities may collect less rigorous methods, leading to lower quality and potential inaccurate data.

How bias might show up in our dataset?

There can be some ways like sampling bias, incompleteness, labeling bias, algorithmic bias, etc. Further to mitigate these bias issues we can adopt data practices like inclusive data collection, transparency, continuous evaluation.

**Step 2:**
Check the dataset for missing values.
"""

#TODO: Leverage the isnull() and sum() functions to find the number of missing values in each column
missing_values = data.isnull().sum()

#TODO: Turn the missing value counts into percentages
missing_values_percentage = (missing_values / len(data)) * 100

#TODO: Return counts and percentages of missing values in each column
missing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage (%)': missing_values_percentage})
missing_data.sort_values(by='Percentage (%)', ascending=False)

"""Here's an overview of the missing values in the dataset:

Columns like VEHICLE TYPE CODE 5, CONTRIBUTING FACTOR VEHICLE 5, VEHICLE TYPE CODE 4, and so on have a high percentage of missing values. This is expected since not all crashes involve multiple vehicles or factors.

OFF STREET NAME and CROSS STREET NAME have significant missing values. This could be due to crashes occurring in locations where these details aren't applicable or weren't recorded.

ZIP CODE, BOROUGH, and ON STREET NAME also have missing values. This might be due to incomplete data entry or crashes occurring in areas where these specifics aren't easily determinable.

LOCATION, LATITUDE, and LONGITUDE have the same count of missing values, indicating that when one is missing, the others are likely missing as well.

**Step 3:** Create a bar chart to display the top 10 contributing factors (e.g. backing up unsafely, unsafe lane changing, etc.) to crashes within the dataset.
"""

#TODO: Plot a Bar Chart

top_factors = data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts().head(10)


plt.figure(figsize=(12, 7))
# TODO: Plotting the top contributing factors, fill in x as the index field of the variable 'top_factors'
sns.barplot(x=top_factors.index, y=top_factors.values, palette="magma")
plt.title('Top 10 Contributing Factors to crashes', fontsize=16)
plt.xlabel('Contributing Factor', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""**TO DO:** Besides for "Unspecified," what are the top 3 contributing factors that cause the most crashes?

> *  Driver Inattention/Distraction
> *  Failure to yield right of way
> *  Following too closely

**TO DO:** What recommendations would you make to new and current drivers after assessing the above data?

> *   Recommendation to new and current drivers can be to educate on distraction awareness like training programs or practical tips. Alos emphasizing safe following distances where they can use three-second rule which allows adequate time to react to vehicle in front, also utilizing technology like collision warning systems or adaptive cruise control. These recommendation can reduce the likelihood of accidents and improve overall road safety.

**Step 4:** Now, let's create another bar chart to determine which vehicle types were involved in the most crashes.
"""

# Determine the top vehicle types involved in crashes
top_vehicle_types = data['VEHICLE TYPE CODE 1'].value_counts().head(10)

# Plotting the top vehicle types
plt.figure(figsize=(12, 7))
sns.barplot(x=top_vehicle_types.index, y=top_vehicle_types.values, palette="cividis")
plt.title('Top 10 Vehicle Types Involved in crashes', fontsize=16)
plt.xlabel('Vehicle Type', fontsize=14)
plt.ylabel('Number of crashes', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""**TO DO:** What are the top 3 vehicles that were most involved in crashes?


> *   Sedan
> *   Station wagon
> *   Passenger vehicle

**TO DO:** Why do you think that "Sedan[s]," "Station Wagon[s]," and "Passenger Vehicle[s]" are involved in a larger number of crashes, injuries, and deaths when compared to the rest of the vehicles? (Think outside the box!)


> *  To start with Sedan, which is more popular with younger drivers, where statistically more likely to engage in risky behavior or have less driving experience. Station wagons and sedan is also passenagers vehicals which states largest portion of privatelt owned vehicles, which can lead to exposure to accidents. Last but not the least, there are more people travelling in passenager vehicles which can result in multiple injuries or deaths compared to smaller capacity vehicles.

**TO DO:** Review the x-axis of the bar chart you created above. </br>
1) What do you notice? </br>
2) What would you recommend we do to improve the bar chart, based on the x-axis (horizontal axis) and why? </br>
3) What recommendation would you make to those who are collecting and/or inputting data into this dataset?


> *  1)  I noticed that some of the top 10 vehicles are repeated, such as "Taxi" appearing more than once, as well as "Station Wagon/Sport Utility Vehicles." This duplication may result from inconsistent use of upper and lower case in the data.
> *  2) We can perform data cleaning by converting the vehicle names into lowercase in a single column to standardize the entries. This will help eliminate duplicate entries caused by case differences. Additionally, we can exclude the top 10 vehicles from the dataset if needed, to focus on less frequent categories or identify other trends more effectively.
> *  3) I recommend standardizing data entry using a single, unified form to improve accuracy. Since multiple individuals contribute to collecting this data, I suggest creating manual guides or instructional sheets for new personnel. These guides would provide step-by-step instructions, ensuring that anyone entering information follows the same process. This approach will help minimize inconsistencies and errors, leading to more reliable and accurate data collection in the future.
"""

data['VEHICLE TYPE CODE 1'] = data['VEHICLE TYPE CODE 1'].str.lower()

# Determine the top vehicle types involved in crashes
top_vehicle_types = data['VEHICLE TYPE CODE 1'].value_counts().head(10)

# Plotting the top vehicle types
plt.figure(figsize=(12, 7))
sns.barplot(x=top_vehicle_types.index, y=top_vehicle_types.values, palette="cividis")
plt.title('Top 10 Vehicle Types Involved in crashes', fontsize=16)
plt.xlabel('Vehicle Type', fontsize=14)
plt.ylabel('Number of crashes', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Note: As I recommended I changed column to lower case and created bar chart which changes top 10 vehicle and give bit more accurate data.

**Step 5:**  A DataFrame is a two-dimensional and potentially heterogeneous tabular data structure with labeled axes (rows and columns). DataFrames allow for storing, manipulating, and retrieving data in a structured form, serving as the primary data structure for a multitude of data processing tasks. It is used with Python libraries like pandas.

Let's graph the *types* of crashes within this dataset and their frequencies. Begin by aggregating your data, convert to [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) for simple plotting, and plot.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Aggregating data - Complete for Cyclist and Motorist
types_of_crashes = {
    'Pedestrian Injuries': data['NUMBER OF PEDESTRIANS INJURED'].sum(),
    'Cyclist Injuries': data['NUMBER OF CYCLIST INJURED'].sum(),
    'Motorist Injuries': data['NUMBER OF MOTORIST INJURED'].sum(),
    'Pedestrian Deaths': data['NUMBER OF PEDESTRIANS KILLED'].sum(),
    'Cyclist Deaths': data['NUMBER OF CYCLIST KILLED'].sum(),
    'Motorist Deaths': data['NUMBER OF MOTORIST KILLED'].sum()
}

# Converting to DataFrame for easier plotting - we want the items in the dictionary, use the items function
crash_types_df = pd.DataFrame(list(types_of_crashes.items()), columns=['crash Type', 'Count'])

# Plot
plt.figure(figsize=(12, 7))
sns.barplot(x='Count', y='crash Type', data=crash_types_df, palette="mako")
plt.title('Types of crashes and Their Frequencies')
plt.xlabel('Count')
plt.ylabel('Type of crash')
plt.tight_layout()
plt.show()

"""**TO DO:** Analyze the chart above. What is a recommendation you might make to the Department of Transportation based on this data?


> *  Since motorist injuries are the highest, department can impleament stricter speed enforcement and traffice measurees. For pedestrian safety improvement department can increase pedestrian crossings and enhacne their visibility with flashing lights or raised crosswalks. Finally,for cyclist bike lanes can expand so that it is separated from vehicle traffic.

---

##<font color='crimson'> **Milestone #3 - Time Series Analysis**</font>
GOAL: The main goal of this milestone is to dive deeper into Time Series Analysis in order to better understand our data's trends over time.

**Step 1:**

Before we jump into Time Series Analysis (TSA), it's important to understand the basics, including Trends, Seasonality, and Residual Components.

Review one or more of the following resources and tell us what you learned about TSA!

*  [Learn about Time Series patterns here](https://otexts.com/fpp2/tspatterns.html)
* [Learn about Time Plots here](https://otexts.com/fpp2/time-plots.html)
*[Learn how to decompose Time Series Data into Trend and Seasonality](https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/)

**TO DO:** Write 3-5 sentences about TSA.
> *  Time series analysis involves studying data points collected or recorded at specific time intervals to uncover patterns, trends, and seasonal variations. The analysis typically includes decomposing the series into components such as trend, seasonality and noise alloowing clearer insights. This method helps in identifying both short-team and long-term fluctuations, providing actionable insights based on historical treads.

**Step 2:** Let's begin by creating a chart that displays the average number of crashes per hour of the day. This will help us understand whether additional factors are contributing to crashes - i.e. rush hour, school dismissal time, night fall, etc.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
file_path = "/content/drive/My Drive/Masters Project/datasets.csv"
data = pd.read_csv(file_path)

# Convert 'CRASH DATE' and 'CRASH TIME' to datetime
data['CRASH DATE'] = pd.to_datetime(data['CRASH DATE'])
data['CRASH TIME'] = pd.to_datetime(data['CRASH TIME'], format='%H:%M')

# Time of Day Analysis
data['Hour of Day'] = data['CRASH TIME'].dt.hour

# Group by 'Hour of Day' and calculate the average number of crashes per hour
average_crashes_per_hour = data.groupby('Hour of Day').size() / data['Hour of Day'].nunique()

# Plot the average number of crashes
plt.figure(figsize=(12, 6))
sns.barplot(x=average_crashes_per_hour.index, y=average_crashes_per_hour.values)
plt.title('Average Number of crashes per Hour of Day')
plt.xlabel('Hour of Day')
plt.ylabel('Average Number of crashes')
plt.xticks(range(0, 24))
plt.show()

"""**TO DO:** Which time of the day sees the most crashes? Why do you think so?

> *  The time of day with the most crashes appears to be around 4 pm, with the highest frequency of accidents occurring between 2 pm and 6 pm. This is likely because it aligns with rush hour, when many people are commuting home from work or school, leading to heavier traffic and an increased likelihood of accidents. In contrast, the lowest crash frequency is around 3 am, with the least crashes happening between 1 am and 5 am. This is likely due to reduced traffic during these early morning hours, when fewer people are on the roads, leading to a lower chance of accidents.

**Step 3:**
Plot a graph to determine how COVID-19 impacted the number of crashes per month, if at all.
"""

# Convert 'CRASH DATE' to datetime format
data['CRASH DATE'] = pd.to_datetime(data['CRASH DATE'])

# Group by month and year to get the number of crashes per month
monthly_crashes = data.groupby(data['CRASH DATE'].dt.to_period("M")).size()

# Plotting the trend over time
plt.figure(figsize=(15, 7))
monthly_crashes.plot()
plt.title('Number of Crashes per Month', fontsize=16)
plt.xlabel('Date', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.tight_layout()
plt.show()

"""**TO DO:** What does your graph tell you about the impact of COVID-19 on the number of crashes per month? Why do you think this occurred?

> *  The graph shows a noticeable decrease in the number of crashes per month during the period affected by COVID-19, particularly in the early months of the pandemic. This decline likely occurred due to widespread lockdowns, remote work policies, and reduced travel, which significantly lowered the number of vehicles on the roads. With fewer cars and less congestion, the likelihood of accidents naturally decreased. Additionally, restrictions on social gatherings and non-essential travel reduced daily commuting, further contributing to the drop in crash numbers during this period.

**Step 4**: Apply time series decomposition to review trends, seasonality, and residuals. New to time series analysis? Review the [Time Series Flashcard video series](https://youtube.com/playlist?list=PLNs9ZO9jGtUAqd0CNKySksPJJaOqYPMvk&feature=shared) here to learn about trends, components, and further analysis!
"""

import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose

# Count the number of crashes per day, group by CRASH DATE
daily_crashes = data.groupby('CRASH DATE').size()

# Set plot style
sns.set(style="darkgrid")

# Plot the daily crashes time series
plt.figure(figsize=(15, 6))
plt.plot(daily_crashes.index, daily_crashes.values, label='Daily crashes')
plt.title('Daily Motor Vehicle Collisions in NYC')
plt.xlabel('Date')
plt.ylabel('Number of Crashes')
plt.legend()
plt.show()

# Decompose the time series
decomposition = seasonal_decompose(daily_crashes, model='additive', period=365)

# Plot the decomposed components
fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))
decomposition.trend.plot(ax=ax1)
ax1.set_title('Trend')
decomposition.seasonal.plot(ax=ax2)
ax2.set_title('Seasonal Components')
decomposition.resid.plot(ax=ax3)
ax3.set_title('Residuals')
plt.tight_layout()
plt.show()

"""The visualizations above provide valuable insights into the time series of daily motor vehicle collisions in New York City:

1. Time Series Plot: This shows the number of daily crashes over time. You might observe long-term trends, seasonal patterns, or significant outliers.

2. Decomposed Components:
  
    2.1 Trend: This graph shows the long-term trend in the data, which can indicate whether crashes are increasing, decreasing, or stable over time.

    2.2 Seasonality: This reveals any regular patterns that repeat over a specific period, such as yearly. It helps identify times of the year with higher or lower crash frequencies.

    2.3 Residuals: These are the irregular components that cannot be attributed to the trend or seasonality. They might include random or unpredictable fluctuations.

**TO DO:** Based on your *trend graph*, are we seeing an increase or a decrease in crashes between 2014 and 2022?

> *  Based on the trend graph, there is a general decrease in the number of crashes between 2014 and 2022. This downward trend suggests improvements in road safety, changes in traffic patterns, or possibly the effects of policies aimed at reducing accidents over this period.


**TO DO:** Based on your *residual graph*, in what year(s) was there a significant unpredicted fluctuation? Why do you think so?

> *  Looking at the residual graph, there is a significant unpredicted fluctuation in 2020. This is likely due to the COVID-19 pandemic, which led to widespread lockdowns, reduced commuting, and less overall road traffic. The unexpected drop in crash numbers reflects the unusual circumstances of that year, which weren't accounted for in typical seasonal or trend patterns.

---

##<font color='crimson'>**Milestone #4 - Geospatial Analysis**</font>
GOAL: The main goal of this milestone is to explore geospatial aspects of the dataset and get comfortable with regional analysis and geospatial visualizations.

**Step 1:** Before beginning this Milestone, we highly recommend that you review the [NSDC Geospatial Analysis Flashcard Video series](https://www.youtube.com/playlist?list=PLNs9ZO9jGtUAX_2g1-OJp8VkmVum6DqqP) if you are new to Geospatial Analysis!

Let's build a bar chart to compare and analyze the number of crashes across the five boroughs: Brooklyn (also known as Kings County), Queens, Manhattan, Bronx, and Staten Island.
"""

#TODO: Plot a bar chart to compare the number of crashes that occurred in each of the five boroughs.
# Set style
sns.set_style("whitegrid")

# Plotting the distribution of crashes by borough
plt.figure(figsize=(12, 7))
# Find the count of unique values of BOROUGHS. Hint: Use value_count function.
borough_count = data['BOROUGH'].value_counts()
sns.barplot(x=borough_count.index , y=borough_count.values, palette="viridis")
plt.title('Distribution of Crashes by Borough', fontsize=16)
plt.xlabel('Borough', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**TO DO:** Which borough has the highest number of crashes? Which borough has the lowest?

> * Highest: Brooklyn
> * Lowest: Staten Island


**TO DO:** Are there any reasons that you think a certain borough has a higher or lower number of crashes? What are some factors that may be causing this?

> *  Brooklyn has the highest number of crashes, while Staten Island has the lowest. Brooklyn is densely populated and has a high volume of both vehicle and pedestrian traffic, leading to more frequent interactions and a greater chance of accidents. Additionally, Brooklyn has many busy intersections and commercial areas, which can increase crash risk.
In contrast, Staten Island is less densely populated and has fewer high-traffic areas, which may reduce the likelihood of accidents.
Factors such as population density, traffic volume, road layout, and commuting patterns likely play key roles in these crash differences between boroughs.

**Step 2:** Heatmaps are graphical representations that use color coding to represent different values and variables. Let's leverage a heatmap to determine the most dangerous intersections in the dataset. (**Note: the below cell may take a few minutes to run**)
"""

#TODO: Create a heatmap leveraging the latitude and longitude variables to determine where the most crashes are occurring
from folium.plugins import HeatMap

# Drop rows with missing latitude and longitude values
data_geo = data.dropna(subset=['LATITUDE', 'LONGITUDE'])

# Create a base map
m = folium.Map(location=[40.730610, -73.935242], zoom_start=10)  # Centered around NYC

# Create a heatmap
heat_data = [[row['LATITUDE'], row['LONGITUDE']] for index, row in data_geo.iterrows()]
HeatMap(heat_data, radius=8, max_zoom=13).add_to(m)

m.save("Heatmap.html")

"""**TO DO:** On the left side of your screen, you will see an icon that represents a folder or a file. Click on that icon to find the file titled "Heatmap.html". Click on the three dots next to your file and download your heatmap! Open the file once downloaded to see your creation.

When looking at your heatmap, where do you see a concentration of crashes?


> *  Upon examining the heatmap, there appears to be a high concentration of crashes in central and southern parts of Brooklyn, as well as in areas of Manhattan, particularly around Midtown and Lower Manhattan. These regions are known for their high population density, significant vehicle and pedestrian traffic, and numerous commercial centers, which can increase the likelihood of accidents.

**Step 3:** Next, we will begin "Severity Mapping." We'll plot crashes on the map and code them based on severity, distinguishing between crashes that resulted in injuries and those that led to fatalities. This will give us a visual representation of where severe crashes tend to occur. </br>

You may initially want to code these incidents by using different colors, but it's important to think about your map and how accessible it is. Will a color-coded map be easy to read for everyone? Let's make a map that's inclusive for people with color blindness by also creating differently-shaped markers (squares, circles, and triangles) for crashes, injuries, and fatalities.
"""

#TODO: Continue building your heatmap
# Sample a subset of the data for visualization
sample_data_severity = data_geo.sample(n=1000, random_state=42)

# Create a base map
m_severity = folium.Map(location=[40.730610, -73.935242], zoom_start=10)

# Add crashes to the map with color coding and shape coding based on severity
for index, row in sample_data_severity.iterrows():
    if row['NUMBER OF PERSONS KILLED'] > 0:
        color = "red"  # Fatalities

        folium.features.RegularPolygonMarker(
          location=[row['LATITUDE'], row['LONGITUDE']],
          number_of_sides=3,
          radius=5,
          gradient = False,
          color=color,
          fill=True,
          fill_color=color
        ).add_to(m_severity)


    elif row['NUMBER OF PERSONS INJURED'] > 0:
        color = "orange"  # Injuries
        folium.CircleMarker(
          location=[row['LATITUDE'], row['LONGITUDE']],
          radius=5,
          color=color,
          fill=True,
          fill_color=color
       ).add_to(m_severity)
    else:
        color = "green"  # No injuries or fatalities
        folium.features.RegularPolygonMarker(
          location=[row['LATITUDE'], row['LONGITUDE']],
          number_of_sides=4,
          radius=5,
          gradient = False,
          color=color,
          fill=True,
          fill_color=color
        ).add_to(m_severity)


m_severity.save("severity.html")

"""**TO DO:** On the left side of your screen, you will see an icon that represents a folder or a file. Follow the same steps as above to download the "Severity.html" file.

**TO DO:** Which intersection(s) seem to be the most dangerous?

> *  After reviewing the file, certain intersections stand out as particularly dangerous, with a high frequency of severe crashes. These are likely to include intersections in high-traffic areas, such as Times Square in Manhattan or Flatbush Avenue and Atlantic Avenue in Brooklyn. These intersections have heavy vehicle and pedestrian traffic, complex road layouts, and often experience congestion, all of which contribute to a higher risk of severe accidents.

---
---

##<font color='crimson'>  **Milestone #5 - Self-Guided Research Question**</font>
GOAL: In this Milestone, you will be prompted to take what youâ€™ve learned throughout this project, build your own research question, and create a visualization(s) or model(s) to support your research goals.

You may create your visualization(s) in this Colab Notebook, or in Excel, Tableau, PowerBI, etc. Choose whichever medium you are most comfortable with! Be creative!

For participants who are comfortable with advanced data science techniques, we welcome you to leverage additional datasets, if desired. We highly recommend using pre-cleaned datasets from open data sources, like Kaggle.com.

If you have any questions or get stuck, please email nsdc@nebigdatahub.org with your queries. We're here to help!

**Step 1:** Review the dataset(s) that you will be using. As you explore, [consider the research question you want to answer](https://libraries.indiana.edu/sites/default/files/Develop_a_Research_Question.pdf)! Additionally, think about [who you are telling your data's story to](https://hbr.org/2013/04/how-to-tell-a-story-with-data). Your final audience may contain a group of transportation professionals, data scientists, peers, and the general public. Think about how would you frame your analysis differently for each of these groups.

**TO DO:** List one or more research questions here that you are considering.

> *  Predictive Analysis: How can we predict the severity of a crash (in terms of injuries and fatalities) based on location, time, and contributing factors?
What locations and times are associated with a higher frequency of severe traffic incidents, and can we predict where future crashes are most likely to occur?

**Step 2:** Now, think about what type of analysis you'd like to complete. Are you interested in looking at time series forecasting? Do you have additional maps in mind that you'd like to create? Is there a certain zip code or region you'd like to dive deeper into?

If you happen to be stuck, here are some examples that you can use or can guide you in choosing your research question!

**Examples:**
- How many crashes, injuries, and/or fatalies occurred in a zip code of interest?
- Which zip code sees the highest amount of crashes and what recommendations can you offer to help that community? Is it an underserved community?
- Do more crashes occur in underrepresented communities? Support your conclusion.
- Which day of the week sees the most crashes, injuries, and/or fatalities? (Hint: use the same method we used when we were analyzing the average number of crashes at different times of the day!)
- Does the geometric features of an intersection (90 degree intersection vs skewed intersection) affect the number of crashes that occur?

Be creative and think outside the box!

**Step 3:** Now that you've decided on your transportation research question, [explore the various types of visualizations you can create to support your research](https://datavizcatalogue.com/). You may create visualizations in this Google Colab notebook, Excel, R, SQL, PowerBI, Tableau, etc. Choose a program you are comfortable with!

You may also choose to build a model or leverage a different data science technique based on your experience level.

**Step 4:** Consider the **accessibility** of the graphs, charts, maps, or models you are interested in building. Use the tools below to learn more!
* How does your visualization appear to people [who may not be able to distinguish between muted colors or see your chart at all?](https://chartability.fizz.studio/)
*[Color Contrast Checker](https://policyviz.com/2022/11/01/color-contrast-checker-in-excel/)
*[SAS Graphics Accelerator](https://support.sas.com/software/products/graphics-accelerator/index.html)
*[TwoTone Data Sonification Tool](https://twotone.io/about/)
*[Making Visual Studio Accessible](https://code.visualstudio.com/docs/editor/accessibility)

To make visualizations more inclusive for people with color blindness, you can choose a color palette that is colorblind-friendly. `Seaborn`, a Python visualization library, provides several palettes that are designed to be perceptible by those with color vision deficiencies. Seaborn's `cubehelix` palette is a good choice, as it was designed specifically with color blindness in mind.

**Step 5:** Begin your research! Give yourself plenty of time to build your visualization or model. If you have any questions along the way, please email nsdc@nebigdatahub.org or write a message in the #[tdsp-community Slack Channel](https://join.slack.com/t/nsdcorps/shared_invite/zt-1h64t1e2p-La0AgU_HhymWUEGFQEcb3w).

**TO DO:** List the research question(s) you've chosen and why! Maybe you chose this question because it can help a community of interest or because it is similar to research you've completed in a class setting. Share your thoughts below.

> *  How can we predict the severity of a crash, in terms of injuries and fatalities, based on location, time, and contributing factors? Identify high-risk locations and times associated with a higher frequency of severe traffic incidents? Also, Can we predict next crash, based on above points like high- risk locations and time, severity of crash?
I chose above question because it allow for actionable insights that could directly benefit communities by helping reduce crash severity and frequency. Predictive analysis of crash severity and high-risk zones could support city planners, transportation departments, and safety advocates in creating data-driven policies and interventions. By identifying patterns related to time, location, and contributing factors, we can help prioritize resources effectively, perhaps enhancing safety infrastructure in specific areas or at high-risk times.

**TO DO:** Build a visualization, model, or use other statistical methods to gain insights into your data and to support your research question.
"""

#TO DO: Begin creating here!
data.head()

"""
Preprocessing the data:
Converting date and time to datetime format.
Extracting additional features (day of the week, hour of day, month).
Encoding categorical variables (e.g., borough, street names, contributing factors).
Scaling continuous variables (e.g., latitude, longitude) if needed.
"""

data['CRASH DATE'] = pd.to_datetime(data['CRASH DATE']).dt.strftime('%Y-%m-%d')
data['CRASH TIME'] = data['CRASH TIME'].astype(str)

data['CRASH DATETIME'] = pd.to_datetime(data['CRASH DATE'] + ' ' + data['CRASH TIME'])

data.drop(columns=['CRASH DATE', 'CRASH TIME'], inplace=True)

data['Day of Week'] = data['CRASH DATETIME'].dt.dayofweek
data['Month'] = data['CRASH DATETIME'].dt.month

# Calculate 'Severity' column
data['Severity'] = 0
data.loc[(data[['NUMBER OF PERSONS INJURED', 'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF CYCLIST INJURED', 'NUMBER OF MOTORIST INJURED']].sum(axis=1) > 0), 'Severity'] = 1
data.loc[(data[['NUMBER OF PERSONS KILLED', 'NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST KILLED', 'NUMBER OF MOTORIST KILLED']].sum(axis=1) > 0), 'Severity'] = 2

# Check the resulting DataFrame
print(data[['CRASH DATETIME', 'Day of Week', 'Hour of Day', 'Month', 'Severity']].head())

"""The goal is to visually analyze the distribution of crash severity based on the day of the week and hour of the day. By using a heatmap, I aim to uncover patterns or trends in crash data that might not be immediately apparent in raw numbers."""

pivot_table = data.pivot_table(values='Severity', index='Day of Week', columns='Hour of Day', aggfunc='count')
plt.figure(figsize=(12, 7))
sns.heatmap(pivot_table, annot=False, cmap='Reds', fmt='.1f', linewidths=1, cbar_kws={'label': 'Severity'})

# Customize the plot
plt.title('Heatmap of Crash Severity by Day of Week and Hour of Day', fontsize=16)
plt.xlabel('Hour of Day', fontsize=14)
plt.ylabel('Day of Week', fontsize=14)
plt.tight_layout()

# Show the plot
plt.show()

"""The heatmap reveals that crashes with higher severity predominantly occur between 3 PM and 6 PM.
This could be attributed to higher traffic volumes during evening rush hours when drivers are commuting from work or school.
The days Wednesday through Friday show a noticeable concentration of severe crashes compared to the rest of the week.
This trend might reflect increased traffic activities closer to the weekend or other behavioral factors.

The purpose is to identify and label geographic regions as "High Risk" or "Low Risk" for crash severity using clustering techniques. This allows for better understanding and prioritization of areas where interventions may be needed to reduce high-severity crashes.
"""

from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

#Select features for clustering
clustering_data = data[['LATITUDE', 'LONGITUDE', 'Severity']].copy()

#Impute missing values using the SimpleImputer (mean strategy for each column)
imputer = SimpleImputer(strategy='mean')
clustering_data[['LATITUDE', 'LONGITUDE', 'Severity']] = imputer.fit_transform(clustering_data[['LATITUDE', 'LONGITUDE', 'Severity']])

scaler = StandardScaler()
clustering_data[['LATITUDE', 'LONGITUDE', 'Severity']] = scaler.fit_transform(clustering_data[['LATITUDE', 'LONGITUDE', 'Severity']])

#Apply K-Means Clustering (2 clusters for high-risk and low-risk)
kmeans = KMeans(n_clusters=2, random_state=42)
clustering_data['Risk Cluster'] = kmeans.fit_predict(clustering_data[['LATITUDE', 'LONGITUDE', 'Severity']])

#Determine which cluster is high-risk and which is low-risk
cluster_severity = clustering_data.groupby('Risk Cluster')['Severity'].mean()
high_risk_cluster = cluster_severity.idxmax()

#Apply the risk level labels to the clustering_data
clustering_data['Risk Level'] = clustering_data['Risk Cluster'].apply(lambda x: 'High Risk' if x == high_risk_cluster else 'Low Risk')

#Merge 'Risk Level' back into the original data
data = data.reset_index()
clustering_data = clustering_data.reset_index()
data = data.merge(clustering_data[['index', 'Risk Level']], on='index', how='left')

print(data[['LATITUDE', 'LONGITUDE', 'Severity', 'Risk Level']].head(15))

grouped = data.groupby('Risk Level').size().reset_index(name='Count')
print(grouped)

print(data[['Risk Level', 'LATITUDE', 'LONGITUDE', 'Day of Week', 'Hour of Day', 'Month']].isnull().sum())

"""This code groups the crash data by "Risk Level" and counts the number of occurrences in each category, summarizing the distribution of crashes between "High Risk" and "Low Risk" zones. The output shows 499,394 crashes in high-risk areas and 1,627,141 crashes in low-risk areas.

This analysis not only provides insights into crash patterns but also creates a foundation for training predictive model. By using this labeled data, machine learning model can develop to predict future crashes and identify potential high-risk areas proactively.
"""

import matplotlib.pyplot as plt
import seaborn as sns

hourly_risk = data.groupby(['BOROUGH', 'Risk Level']).size().reset_index(name='Count')

plt.figure(figsize=(10, 8))
sns.lineplot(data=hourly_risk, x='BOROUGH', y='Count', hue='Risk Level', marker='o', palette="viridis")

plt.title('Distribution of Crashes by Risk Level', fontsize=16)
plt.xlabel('BOROUGH', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.xticks(range(0, 5))
plt.legend(title='Risk Level')
plt.tight_layout()
plt.show()

"""This visualizes the distribution of crashes by borough and risk level using a line plot. The plot is useful for traffic management authorities to compare the relative safety of boroughs and focus on boroughs with a high count of severe crashes for risk mitigation efforts."""

import pandas as pd
import folium
from folium.plugins import HeatMap

boroughs = ['MANHATTAN', 'BROOKLYN', 'QUEENS', 'BRONX', 'STATEN ISLAND']

#Remove rows with missing values in relevant columns
data = data.dropna(subset=['LATITUDE', 'LONGITUDE', 'Risk Level'])

#Create a Folium Map centered on NYC
map_center = [data['LATITUDE'].mean(), data['LONGITUDE'].mean()]
m = folium.Map(location=map_center, zoom_start=11)

#Loop over each borough and plot heatmaps for high and low risk
for borough in boroughs:
    borough_data = data[data['BOROUGH'] == borough]

    if borough_data.empty:
        continue

    high_risk_borough = borough_data[borough_data['Risk Level'] == 'High Risk']
    low_risk_borough = borough_data[borough_data['Risk Level'] == 'Low Risk']

    low_risk_heat_data = [[row['LATITUDE'], row['LONGITUDE']] for index, row in low_risk_borough.iterrows()]
    high_risk_heat_data = [[row['LATITUDE'], row['LONGITUDE']] for index, row in high_risk_borough.iterrows()]

    if low_risk_heat_data:
        HeatMap(
            low_risk_heat_data,
            min_opacity=0.4,
            radius=10,
            gradient={0.2: 'blue', 1: 'darkblue'},
            name=f'{borough} - Low Risk'
        ).add_to(m)

    if high_risk_heat_data:
        HeatMap(
            high_risk_heat_data,
            min_opacity=0.4,
            radius=10,
            gradient={0.2: 'orange', 1: 'red'},
            name=f'{borough} - High Risk'
        ).add_to(m)

folium.LayerControl().add_to(m)
m

"""This map shows low to high risk level on map which can further map us to train our model. As this can also help us to understand where is high risk areas and we can take cautions to prevent those crashes

Since we have already incorporated severity into our analysis, we can enhance the model's accuracy by including additional factors, such as contributing factors. These factors, often represented as textual or categorical data, provide valuable context about the causes of crashes. Leveraging this information can help the model better understand crash dynamics and improve its predictive performance.
"""

grouped = data.groupby('CONTRIBUTING FACTOR VEHICLE 1').size().reset_index(name='Count')
print(grouped)

category_mapping = {
    'Driver Behavior': ['Aggressive Driving/Road Rage', 'Alcohol Involvement', 'Drugs (illegal)', 'Fatigued/Drowsy', 'Texting', 'Driver Inattention/Distraction'],
    'Vehicle Defects': ['Accelerator Defective', 'Brakes Defective', 'Headlights Defective', 'Tire Failure/Inadequate', 'Steering Failure'],
    'External Factors': ['Animals Action', 'Obstruction/Debris', 'Weather', 'Glare', 'Pedestrian/Bicyclist/Other Pedestrian Error'],
    'Inexperience': ['Driver Inexperience', 'Reaction to Uninvolved Vehicle'],
    'Vehicle Control': ['Backing Unsafely', 'Failure to Yield Right-of-Way', 'Unsafe Speed', 'Unsafe Lane Changing'],
    'Distractions': ['Cell Phone (hand-Held)', 'Listening/Using Headphones', 'Passenger Distraction', 'Outside Car Distraction'],
    'Traffic Violations': ['Failure to Keep Right', 'Passing Too Closely', 'Passing or Lane Usage Improper', 'Turning Improperly', 'Traffic Control Disregarded'],
    'Road Conditions': ['Pavement Defective', 'Pavement Slippery', 'Lane Marking Improper/Inadequate', 'Traffic Control Device Improper/Non-Working'],
    'Vehicle Issues': ['Other Vehicular', 'Other Lighting Defects', 'Other Electronic Device', 'Vehicle Vandalism'],
    'Miscellaneous': ['Unspecified', 'Illness', 'Eating or Drinking', 'Prescription Medication', 'Windshield Inadequate']
}

def categorize(row):
    for category, factors in category_mapping.items():
        if row in factors:
            return category
    return 'Other'

data['Contributing Factors'] = data['CONTRIBUTING FACTOR VEHICLE 1'].apply(categorize)

print(data[['Contributing Factors']].head(15))

"""Categorizing the 10 most common contributing factors and organizing them into defined categories can help systematically account for all contributing factors. This approach ensures that critical factors are acknowledged effectively, providing a structured framework for analysis and improving the model's overall accuracy and interpretability."""

contributing_factors_mapping = {
    'Distractions': 0,
    'Driver Behavior': 1,
    'External Factors': 2,
    'Inexperience': 3,
    'Traffic Violations': 4,
    'Vehicle Control': 5,
    'Vehicle Defects': 6,
    'Vehicle Issues': 7,
    'Road Conditions': 8,
    'Miscellaneous': 9,
    'Other': 9
}
data['Contributing Factors Num'] = data['Contributing Factors'].map(contributing_factors_mapping)

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
from imblearn.over_sampling import SMOTE

# Map target variable
data['Risk Level'] = data['Risk Level'].map({'Low Risk': 0, 'High Risk': 1})

# Select features and target
features = ['LATITUDE', 'LONGITUDE', 'Day of Week', 'Hour of Day', 'Month', 'Contributing Factors Num']
X = data[features]
y = data['Risk Level']

# Randomly sample 25% of the data for training
sampled_data = data.sample(frac=0.25, random_state=42)
X_sampled = sampled_data[features]
y_sampled = sampled_data['Risk Level']

# Split the sampled data
X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.3, random_state=42)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Initialize XGBoost model
model_xgb_risk = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Define a parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Use GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(model_xgb_risk, param_grid, scoring='f1', cv=3, verbose=1, n_jobs=-1)
grid_search.fit(X_train_resampled, y_train_resampled)

# Train the model with the best parameters
best_model = grid_search.best_estimator_
best_model.fit(X_train_resampled, y_train_resampled)

# Add predictions to the dataset
data['Predicted Risk Level'] = best_model.predict(X)

# Save the best parameters for reference
# print("Best Parameters:\n", grid_search.best_params_)

# Evaluate the model
predictions = best_model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
report = classification_report(y_test, predictions)

print(f"Accuracy of the XGBoost Risk Level model: {accuracy * 100:.2f}%")
print("Classification Report:\n", report)

"""Now that we have all the necessary columns, we can define X (input features) and Y (output target) for training our model. I used the XGBoost algorithm to train the model and predict future crashes.

Why XGBoost?

XGBoost is a powerful and efficient gradient boosting algorithm known for its ability to handle large datasets and complex relationships in the data. Some key benefits of using XGBoost include:

High Performance: XGBoost is optimized for speed and can handle large-scale data efficiently.

Accuracy: It has a proven track record of achieving high predictive accuracy in various machine learning competitions.

Handling Missing Data: XGBoost can handle missing data without requiring explicit imputation.

Feature Importance: It provides insights into the importance of different features, helping us understand which factors most influence crash predictions.

Flexibility: XGBoost allows tuning hyperparameters for better model performance and can be used for both regression and classification tasks.

Using XGBoost will allow us to build a robust model that can accurately predict future crashes based on the contributing factors, helping improve safety measures and traffic management.
"""

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split

data['Risk Level'] = data['Risk Level'].map({'Low Risk': 0, 'High Risk': 1})

# Select features and target
features = ['LATITUDE', 'LONGITUDE', 'Day of Week', 'Hour of Day', 'Month', 'Contributing Factors Num']
X = data[features]
y = data['Risk Level']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model_xgb_risk = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')

model_xgb_risk.fit(X_train, y_train)

accuracy = model_xgb_risk.score(X_test, y_test)
print(f"Accuracy of the XGBoost Risk Level model: {accuracy * 100:.2f}%")

data['Predicted Risk Level'] = model_xgb_risk.predict(X)

data['Predicted Risk Level'] = data['Predicted Risk Level'].map({0: 'Low Risk', 1: 'High Risk'})

from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score

# Make predictions on the test set
y_pred = model_xgb_risk.predict(X_test)

# Compute metrics
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Print metrics
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"F1 Score: {f1:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")

# Classification report for more detailed metrics
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred, target_names=['Low Risk', 'High Risk']))

grouped = data.groupby('Predicted Risk Level').size().reset_index(name='Count')

print(grouped)

"""After training our model we have created predicted risl level column to predict next possible crash in different location. As we can see in output above we got high risk 1002 location which also include contributing factor for that crash.

Displaying high risk levels on map to understand the locations much better
"""

import folium
from folium.plugins import HeatMap

map_center = [data['LATITUDE'].mean(), data['LONGITUDE'].mean()]
m = folium.Map(location=map_center, zoom_start=11)

heat_data = [[row['LATITUDE'], row['LONGITUDE']] for idx, row in data.iterrows() if row['Predicted Risk Level'] == 'High Risk']

if heat_data:
    HeatMap(heat_data, min_opacity=0.4, radius=10, gradient={0.2: 'blue', 1: 'red'}).add_to(m)

m

"""
As we visualize the data using a color gradient from blue to red, we can observe patterns that indicate the likelihood of future crashes. The transition from blue to red highlights areas with increasing severity or risk, helping us predict and prioritize locations where future crashes are more likely to occur."""

import folium
from folium.plugins import HeatMap

map_center = [data['LATITUDE'].mean(), data['LONGITUDE'].mean()]
m = folium.Map(location=map_center, zoom_start=11)

heat_data = []
for idx, row in data.iterrows():
    if row['Predicted Risk Level'] == 'High Risk':
        heat_data.append([row['LATITUDE'], row['LONGITUDE']])

        folium.CircleMarker(
            location=[row['LATITUDE'], row['LONGITUDE']],
            radius=5,
            color="red",
            fill=True,
            fill_opacity=0,
            popup=f"Contributing Factors: {row['Contributing Factors']}"
        ).add_to(m)

if heat_data:
    HeatMap(heat_data, min_opacity=0.4, radius=10, gradient={0.2: 'blue', 1: 'red'}).add_to(m)
m

"""This creates an interactive map that visualizes high-risk crash locations. Each high-risk crash point is marked with a red circle on the map, and clicking on the marker reveals the contributing factors associated with that crash.

By analyzing the map and understanding the contributing factors, we can take actionable steps to improve traffic safety.

For example: Road Conditions as a Contributing Factor:
If the map reveals that many high-risk crashes occur in areas with poor road conditions (e.g., potholes, slippery surfaces, or construction zones), authorities can prioritize these locations for maintenance and repairs.

Action:
Suppose we notice a cluster of high-risk crashes in a particular area during rainy weather. The contributing factor might be wet roads or poor drainage. By using this information, traffic management can implement timely interventions, such as better road surfacing, enhanced signage, or increased road maintenance in those areas, to reduce the likelihood of future crashes.

In summary, this map serves as a tool to identify areas where specific contributing factors, like road conditions, are leading to high-risk crashes. By addressing these factors proactively, we can reduce future crashes and improve overall road safety.
"""

high_risk_data = data[data['Predicted Risk Level'] == 'High Risk']
high_risk_table = high_risk_data[['LATITUDE', 'LONGITUDE', 'Day of Week', 'Hour of Day', 'Month', 'Contributing Factors']]

# Display the table
high_risk_table

"""### **Conclusion:**

Based on the predictive analysis using the XGBoost model, we achieved an accuracy of 76.40% in predicting high-risk crash locations. This model has effectively identified high-risk areas, which can help prioritize safety interventions. The interactive map, visualizing contributing factors, further supports decision-making by highlighting areas needing improvement, such as road condition maintenance.

To improve accuracy, we can enhance the model by adding more relevant features like weather conditions, traffic volume, and time of day. Hyperparameter tuning using grid search or random search can also optimize model performance. Addressing imbalanced data with techniques like SMOTE or class weighting can further improve predictions, particularly for high-risk crashes. Additionally, implementing k-fold cross-validation will ensure the model generalizes better across the data.

### **Future Improvements**

Future improvements could include incorporating temporal and spatial data for more accurate predictions, exploring other machine learning models like LightGBM or Random Forests, and integrating real-time data such as traffic or weather conditions. Lastly, enhancing model interpretability using tools like SHAP or LIME will help ensure the model is making decisions based on the most relevant features, leading to more reliable predictions.

##<font color='crimson'>**Milestone #6 - Virtual Poster Board Creation: Data Storytelling**</font>

GOAL: The main goal of this milestone is to create a one page, virtual poster board to portray your research findings and recommendations! Your poster may be shared with the Department of Transportation and Federal Highway Authority.

Within your poster, summarize your research question, your reasoning for selecting your data visualization or model choices, and key insights from your data analysis. You may also wish to include your outstanding research questions that could not be answered by the dataset and why.

**Be sure to answer the following on your virtual poster board:** Based on your research insights, what recommendations would you share with the Department of Transportation and Federal Highway Authority to make roads safer for vulnerable road users? Why?

**Additionally, be sure to cite all relevant sources that you referred to throughout this project on your poster board (MLA or APA citation preferred). List acknowlegdments if you received any support from mentors, professors, professionals, etc. throughout your journey.**

Please use the following resources to get started!


*   [Virtual Poster Board Template](https://nebigdatahub.org/wp-content/uploads/2024/01/Copy-of-dsi-poster.ppt-48-Ã—-36-in.pdf) - Your one-page, virtual poster may be created in PowerPoint, Google Slides, Canva, etc. Choose what you are most comfortable with!
* [ Data Storytelling: How to Effectively Tell a Story with Data](https://online.hbs.edu/blog/post/data-storytelling)

* [  Consider how your visualization(s) might appear to people with varying abilities ](https://chartability.fizz.studio/)
*  [Understand your audience for an optimal presentation](https://hbr.org/2013/04/how-to-tell-a-story-with-data)


Once completed, please use the [following TDSP Submission Form](https://docs.google.com/forms/d/e/1FAIpQLSeX1OSHj58EQs4ypFEPB_SH3OpWZeo67yU0WWOPVSqYtDrpWg/viewform) to share your Google Colab Notebook and your one-page, virtual project poster with the NSDC HQ Team.

---
---

## ðŸš—<font color='crimson'> **Thank you for completing the project!**</font> ðŸš—

We are one step closer to making roads safer for all. [Please submit all materials to the NSDC HQ team](https://docs.google.com/forms/d/e/1FAIpQLSeX1OSHj58EQs4ypFEPB_SH3OpWZeo67yU0WWOPVSqYtDrpWg/viewform) in order to receive a certificate of completion. Do reach out to us if you have any questions or concerns. We are here to help you learn and grow.
"""